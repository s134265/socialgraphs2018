{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is our data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project it was decided to work with IMDB data. \n",
    "IMDB provides a subset of their data to everyone who might be interesting in analyzing it. The data provided by IMDB are split into multiple files, not all of these files are used for this project.\n",
    "The files used for this project is:\n",
    "name.basics.tsv.gz\n",
    "title.akas.tsv.gz\n",
    "title.basics.tsv.gz\n",
    "title.crew.tsv.gz\n",
    "title.episode.tsv.gz\n",
    "title.principals.tsv.gz\n",
    "title.ratings.tsv.gz\n",
    "The file \"title.principals\" is the main datafile, which orignally consists of 1.3 Gb of data with 30.674.812 rows and 6 columns.\n",
    "These datafiles contains information about movies, actors in those movies, when they movies were made, what the different peoples roles were in the movies, ID of movies and actors, type of movie such as tv show or movie and ratings of these movies.\n",
    "\n",
    "Besides these data files, some files containing reviews of movies were also used. These reviews were downloaded from the website Kaggle.com and consists of 100.000 reviews on 14.127 movies.\n",
    "The reviews from Kaggle are divided into two folders, each containing 50.000 reviews. Sentiment analysis has already been conducted on some of the reviews, however this was ignored for this project. \n",
    "Besides the moview reviews the data also contains URLS describing which movie the different reviews come from, this is the part linking the movies with the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why did you chose this/ these particular datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets chosen for this project provides with a way to link actors to each other through their movies. Besides this it also provides reviews for the movies, such that sentiment analysis can be done on these in order to link the sentiment score of the movies actors has been in, to the actors. \n",
    "\n",
    "Some of the data files are also used when cleaning the data and making it more suitable for the project, this was especially important since the data set was very large initially.\n",
    "\n",
    "All of these data files therefore provides everything needed in order to make this project, which is why they were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to find communities of actors/actresses which are enjoyable together. The project is therefore not about find good movies, but instead finding out which actors/actresses make good movies when working together.\n",
    "\n",
    "It is therefore possible for an actor to have bad reviews in general, but still being enjoyable to watch when paired up with certain actors/actresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two type of data:\n",
    "* reviews\n",
    "* IMBD databases containing actors, movies and rating <br>\n",
    "\n",
    "The databases contains alot of irrelevant information such as games and movies with no reviews in the review data set. Therefore we first have to clean our databases in order to keep only the relevant information.\n",
    "\n",
    "In this we also comment the different methods and tools used for data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "startFromCleanData = True #Start with the raw data imported or the cleaned files\n",
    "fastExecution = False     #Use the stored graph, position and DF of rebuild them\n",
    "savingFigures = True      #Whether to save or not the figures produced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import fa2\n",
    "import math\n",
    "import community\n",
    "import matplotlib.cm as cm\n",
    "from __future__ import division\n",
    "import matplotlib.image as mpimg\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import io\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from scipy.special import zeta\n",
    "import pickle\n",
    "# Rendering Parameters\n",
    "title_font = {'family': 'sans-serif',\n",
    "        'color':  '#000000',\n",
    "        'weight': 'normal',\n",
    "        'size': 16,\n",
    "        }\n",
    "#COLORS\n",
    "mBlue = \"#55638A\"     # For actor\n",
    "fRed = \"#9E1030\"    # For actress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICKLE\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use Pickle as our data frame, because it is native to pandas and also because the Pickle data frame structure is more compressed than txt files and allows for much faster reading of files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Actors and Movie Dictionnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Dictionnary initialised\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Initialise a movie dictionnary\n",
    "###################################\n",
    "\n",
    "# Function. to convert movie or actor id to sting key\n",
    "def idToString(iD, base): # base = \"tt\" for movies or \"nm\" for actors\n",
    "    if iD<10:\n",
    "        return base+\"000000\"+str(iD)\n",
    "    if iD<100:\n",
    "        return base+\"00000\"+str(iD)\n",
    "    if iD<1000:\n",
    "        return base+\"0000\"+str(iD)\n",
    "    if iD<10000:\n",
    "        return base+\"000\"+str(iD)\n",
    "    if iD<100000:\n",
    "        return base+\"00\"+str(iD)\n",
    "    if iD<1000000:\n",
    "        return base+\"0\"+str(iD)\n",
    "    else:\n",
    "        return base+str(iD)\n",
    "    \n",
    "# Create movie dictionnary\n",
    "movieDict = {}\n",
    "lastMovie = 9999999 #last movie ID\n",
    "if not fastExecution:\n",
    "    for i in range(lastMovie):\n",
    "        movieDict[idToString(i+1,\"tt\")] = False\n",
    "    print \"Movie Dictionnary initialised\"\n",
    "else:\n",
    "    print \"Fast execution mode, movie dictionnary will be initialised later\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use dictionaries, when checking which movies and actors we want to save in our data. This is because dictionaries provide an easy way to save each movie ID and actor ID as a key in the dictionary and having the value of that key as either True or false, depending if it should be kept or not.\n",
    "Another reason why we decided on using dictionaries are that dictionaries is faster to use when running code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Get the movies to keep\n",
    "###################################\n",
    "\n",
    "# List of the reviews documents\n",
    "listReviewsDocuments = [\"train/urls_neg.txt\",\"test/urls_neg.txt\",\"train/urls_pos.txt\",\"test/urls_pos.txt\",\"train/urls_unsup.txt\"]\n",
    "\n",
    "# Fill in the dictionnary\n",
    "for document in listReviewsDocuments:\n",
    "    files = io.open(\"aclImdb/\"+document, mode=\"r\", encoding=\"utf-8\")\n",
    "    for row in files:\n",
    "        w = re.findall(r'http://www.imdb.com/title/(\\w*)/usercomments',row)\n",
    "        movieDict[w[0]] = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughtout this project all data is encoded and decoded with unicode. The reason for this is that the data used for this project is already encoded in unicode. It is therefore the obvious choice to keep the same formate when handling the text, throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor Dictionnary initialised\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Create an Actor Dict\n",
    "###################################\n",
    "actorDict = {}\n",
    "lastActor = 29999999 #last movie ID\n",
    "for i in range(lastActor):\n",
    "    actorDict[idToString(i+1,\"nm\")] = False\n",
    "print \"Actor Dictionnary initialised\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this setup the data is ready to be cleaned.\n",
    "The way the data was cleaned was to only save data which is relevant for this project. First it was relevant to only save movies which has reviews and which are actually movies and not games, tv shows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# key to movie name file\n",
    "###################################\n",
    "\n",
    "if not startFromCleanData:\n",
    "    path = \"DATA/title.basics.txt\"\n",
    "    cleanPath = \"DATA/title.basics.clean.txt\"\n",
    "    files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    cleanfile = io.open(cleanPath, mode=\"w\", encoding=\"utf-8\")\n",
    "    b=False # skip the first line\n",
    "    count =0\n",
    "    for row in files:\n",
    "        if b:\n",
    "            split=row.split(\"\\t\")\n",
    "            key = split[0]\n",
    "            if movieDict[key]:\n",
    "                if (split[1] in ['movie', 'tvMovie']):\n",
    "                    cleanfile.write(row)\n",
    "                    count +=1\n",
    "                else:\n",
    "                    movieDict[key]=False\n",
    "        else:\n",
    "            b=True\n",
    "    files.close()\n",
    "    cleanfile.close()\n",
    "\n",
    "\n",
    "    print \"There are \"+str(count)+\" movies considered\"\n",
    "    print \"DATA/title.basics.txt cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, only actors and actresses in the remaining movies should be saved, everyone not in the movies or with another role than actor/actress where therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# film actors links file : Clean + get actor dictionnary\n",
    "##########################################################\n",
    "\n",
    "if not startFromCleanData:\n",
    "    path = \"DATA/title.principals.txt\"\n",
    "    cleanPath = \"DATA/title.principals.clean.txt\"\n",
    "    files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    cleanfile = io.open(cleanPath, mode=\"w\", encoding=\"utf-8\")\n",
    "    roleCheckList = [\"actor\", \"actress\", \"self\"] #check if it is an actor\n",
    "    nLinks = 0\n",
    "    i=False # skip first line\n",
    "    for row in files:\n",
    "        if i:\n",
    "            split = row.split(\"\\t\") \n",
    "            key = split[0]\n",
    "            if movieDict[key]:\n",
    "                if (split[3] in roleCheckList or split[4] in roleCheckList or split[5] in roleCheckList):\n",
    "                    cleanfile.write(row)\n",
    "                    actorDict[split[2]]=True\n",
    "                    nLinks  +=1\n",
    "\n",
    "        else:\n",
    "            i=True\n",
    "\n",
    "    files.close()\n",
    "    cleanfile.close()\n",
    "\n",
    "    ##REMOVE ERRORS\n",
    "    actorDict[\"nm0547707\"]=False\n",
    "    actorDict['nm0547707']=False\n",
    "    actorDict['nm0809728']=False\n",
    "    actorDict['nm2442859']=False\n",
    "    actorDict['nm1996613']=False\n",
    "    actorDict['nm0600636']=False\n",
    "    actorDict['nm1824417']=False\n",
    "    actorDict['nm2440192']=False\n",
    "    actorDict['nm1754167']=False\n",
    "\n",
    "    print \"There are \"+str(nLinks-9)+\" actors considered\"\n",
    "    print \"DATA/title.principals.txt cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# key to actor name file\n",
    "###################################\n",
    "\n",
    "if not startFromCleanData:\n",
    "    path = \"DATA/name.basics.txt\"\n",
    "    cleanPath = \"DATA/name.basics.clean.txt\"\n",
    "    files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    cleanfile = io.open(cleanPath, mode=\"w\", encoding=\"utf-8\")\n",
    "    count = 0\n",
    "    i=False\n",
    "    for row in files:\n",
    "        if i:\n",
    "            split = row.split(\"\\t\")\n",
    "            key = split[0]\n",
    "            if actorDict[key]:\n",
    "                cleanfile.write(row)\n",
    "        else:\n",
    "            i=True\n",
    "\n",
    "    files.close()\n",
    "    cleanfile.close()\n",
    "    print \"DATA/name.basics.txt cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything not relevant for the project has been removed and only relevant movies and actors/acresses remain, it is then necessary to initialise all of this data, in order to gather relevant information about the data such as movie years etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10735 movies considered\n",
      "Movie Dictionnary Preprocessed and Movie Age Dictionnary Built\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Preprocess Movie Dict and get movie years\n",
    "############################################\n",
    "\n",
    "movieAgeDict = {}\n",
    "\n",
    "path = \"DATA/title.basics.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count =0\n",
    "for row in files:\n",
    "    split=row.split(\"\\t\")\n",
    "    key = split[0]\n",
    "    if movieDict[key]:\n",
    "        if (split[1] in ['movie', 'tvMovie']) and not (split[5] == \"\\\\N\"):\n",
    "            movieAgeDict[key] = int(split[5])\n",
    "            count +=1\n",
    "files.close()\n",
    "\n",
    "#Clean Movie dict\n",
    "for i in range(lastMovie):\n",
    "    movieDict[idToString(i+1,\"tt\")] = False\n",
    "\n",
    "for key in movieAgeDict.keys():\n",
    "    movieDict[key]=True\n",
    "\n",
    "\n",
    "print \"There are \"+str(count)+\" movies considered\"\n",
    "print \"Movie Dictionnary Preprocessed and Movie Age Dictionnary Built\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43553 actors considered\n",
      "Actor Dictionnary Preprocessed\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "# film actors links file : Clean + get actor dictionnary\n",
    "##########################################################\n",
    "\n",
    "path = \"DATA/title.principals.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "roleCheckList = [\"actor\", \"actress\", \"self\"] #check if it is an actor\n",
    "nLinks = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\") \n",
    "    key = split[0]\n",
    "    if movieDict[key]:\n",
    "        if (split[3] in roleCheckList or split[4] in roleCheckList or split[5] in roleCheckList):\n",
    "            actorDict[split[2]]=True\n",
    "            nLinks  +=1\n",
    "\n",
    "files.close()\n",
    "\n",
    "###REMOVE ERRORS\n",
    "actorDict[\"nm0547707\"]=False\n",
    "actorDict['nm0547707']=False\n",
    "actorDict['nm0809728']=False\n",
    "actorDict['nm2442859']=False\n",
    "actorDict['nm1996613']=False\n",
    "actorDict['nm0600636']=False\n",
    "actorDict['nm1824417']=False\n",
    "actorDict['nm2440192']=False\n",
    "actorDict['nm1754167']=False\n",
    "\n",
    "print \"There are \"+str(nLinks-9)+\" actors considered\"\n",
    "\n",
    "print \"Actor Dictionnary Preprocessed\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a ratings dict\n",
    "###################################\n",
    "ratingDict = {}\n",
    "path = \"DATA/ratings.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count = 0\n",
    "i=False # skip first line\n",
    "for row in files:\n",
    "    if i:\n",
    "        key = row[:9]\n",
    "        if movieDict[key]:\n",
    "            split = row.split(\"\\t\") \n",
    "            ratingDict[key] = float(split[1])\n",
    "    else:\n",
    "        i=True\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a movie name dict\n",
    "###################################\n",
    "movieNameDict = {}\n",
    "moviesList = []\n",
    "path = \"DATA/title.akas.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\") \n",
    "    if movieDict[split[0]] and not (split[0] in movieNameDict) and (split[0] in ratingDict) and \"original\" in row   :\n",
    "        movieNameDict[split[0]] = split[2]\n",
    "        moviesList.append(split[0])\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create an actor name dict\n",
    "###################################\n",
    "actorNameDict = {}\n",
    "actorGenderDict = {}\n",
    "actorsList = []\n",
    "path = \"DATA/name.basics.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\") \n",
    "    if actorDict[split[0]] and not (split[0] in actorNameDict):\n",
    "        actorNameDict[split[0]] = split[1]\n",
    "        if \"actor\" in split[4]:\n",
    "            actorGenderDict[split[0]] = \"M\"\n",
    "        else:\n",
    "            actorGenderDict[split[0]] = \"F\"\n",
    "        actorsList.append(split[0])\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>iD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8686</th>\n",
       "      <td>9.1</td>\n",
       "      <td>The Regard of Flight</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>tt0134050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Notre-Dame de Paris</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>tt0285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>8.9</td>\n",
       "      <td>Ko to tamo peva</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>tt0076276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>8.9</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>tt0050083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9860</th>\n",
       "      <td>8.9</td>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>tt0108052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>8.9</td>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>tt0167260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8305</th>\n",
       "      <td>8.8</td>\n",
       "      <td>Saban Oglu Saban</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>tt0253614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>8.8</td>\n",
       "      <td>Sobache serdtse</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>tt0096126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>8.8</td>\n",
       "      <td>The Art of Amália</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>tt0204839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9079</th>\n",
       "      <td>8.8</td>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>tt0120737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating                                              Title    Year  \\\n",
       "8686     9.1                               The Regard of Flight  1983.0   \n",
       "7737     9.0                                Notre-Dame de Paris  1999.0   \n",
       "8377     8.9                                    Ko to tamo peva  1980.0   \n",
       "4887     8.9                                       12 Angry Men  1957.0   \n",
       "9860     8.9                                   Schindler's List  1993.0   \n",
       "1157     8.9      The Lord of the Rings: The Return of the King  2003.0   \n",
       "8305     8.8                                   Saban Oglu Saban  1977.0   \n",
       "1389     8.8                                    Sobache serdtse  1988.0   \n",
       "2151     8.8                                  The Art of Amália  2000.0   \n",
       "9079     8.8  The Lord of the Rings: The Fellowship of the Ring  2001.0   \n",
       "\n",
       "             iD  \n",
       "8686  tt0134050  \n",
       "7737  tt0285800  \n",
       "8377  tt0076276  \n",
       "4887  tt0050083  \n",
       "9860  tt0108052  \n",
       "1157  tt0167260  \n",
       "8305  tt0253614  \n",
       "1389  tt0096126  \n",
       "2151  tt0204839  \n",
       "9079  tt0120737  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# Build a movie data frame\n",
    "###################################\n",
    "if not fastExecution:\n",
    "    moviesData = {\"iD\" : movieNameDict.keys(), \"Title\": pd.Series(np.zeros(len(moviesList))), \"Rating\":pd.Series(np.zeros(len(moviesList))), \"Year\":pd.Series(np.zeros(len(moviesList)))}\n",
    "    moviesDF = pd.DataFrame(moviesData)\n",
    "    for i in moviesDF.index:\n",
    "        iD =moviesDF.loc[i].at[\"iD\"]\n",
    "        moviesDF.loc[i, \"Title\"]= movieNameDict[iD]\n",
    "        moviesDF.loc[i, \"Rating\"] = ratingDict[iD]\n",
    "        moviesDF.loc[i, \"Year\"]= movieAgeDict[iD]\n",
    "    if savingFigures:\n",
    "        moviesDF.to_pickle(\"obj/moviesDF.pkl\")\n",
    "else:\n",
    "    moviesDF = pd.read_pickle(\"obj/moviesDF.pkl\")\n",
    "moviesDF.sort_values(\"Rating\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the data has been cleaned, the remaining data for the movies are their rating, movie title, which year it was made and the movie ID. \n",
    "\n",
    "This data is everything needed in order to link it to the actors and the different reviews as well as categorizing them after year and analyzing ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Name</th>\n",
       "      <th>iD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>Bobbie Bresee</td>\n",
       "      <td>nm0107679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>Malgorzata Rozniatowska</td>\n",
       "      <td>nm0747647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>Ahmet Ugurlu</td>\n",
       "      <td>nm0880128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>Laura Nativo</td>\n",
       "      <td>nm1137466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>Jordy Benattar</td>\n",
       "      <td>nm0070237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>Özkan Ugur</td>\n",
       "      <td>nm0880126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>John Foss</td>\n",
       "      <td>nm1458561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>M</td>\n",
       "      <td>Panayiotis Hartomatzidis</td>\n",
       "      <td>nm0367186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>M</td>\n",
       "      <td>Simon Abkarian</td>\n",
       "      <td>nm0008787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F</td>\n",
       "      <td>Victoria Snow</td>\n",
       "      <td>nm0795281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender                      Name         iD\n",
       "0      F             Bobbie Bresee  nm0107679\n",
       "1      F   Malgorzata Rozniatowska  nm0747647\n",
       "2      M              Ahmet Ugurlu  nm0880128\n",
       "3      F              Laura Nativo  nm1137466\n",
       "4      F            Jordy Benattar  nm0070237\n",
       "5      M                Özkan Ugur  nm0880126\n",
       "6      M                 John Foss  nm1458561\n",
       "7      M  Panayiotis Hartomatzidis  nm0367186\n",
       "8      M            Simon Abkarian  nm0008787\n",
       "9      F             Victoria Snow  nm0795281"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################\n",
    "# Build an actor data frame\n",
    "###################################\n",
    "if not fastExecution:\n",
    "    actorsData = {\"iD\": actorNameDict.keys(), \"Name\": pd.Series(np.zeros(len(actorsList))),\"Gender\": pd.Series(np.zeros(len(actorsList)))}\n",
    "    actorsDF = pd.DataFrame(actorsData)\n",
    "    for i in actorsDF.index:\n",
    "        iD = actorsDF.loc[i].at[\"iD\"]\n",
    "        actorsDF.loc[i, \"Name\"]= actorNameDict[iD]\n",
    "        actorsDF.loc[i, \"Gender\"] = actorGenderDict[iD]\n",
    "    if savingFigures:\n",
    "        actorsDF.to_pickle(\"obj/actorsDF.pkl\")\n",
    "else:\n",
    "    actorsDF = pd.read_pickle(\"obj/actorsDF.pkl\")\n",
    "actorsDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For actors and actresses the only relevant information was their gender, name and IMDB ID which is used when linking them to the movies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a links list\n",
    "###################################\n",
    "path = \"DATA/title.principals.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "links = np.empty((nLinks,2),dtype=object)\n",
    "count = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\")\n",
    "    if actorDict[split[2]]:\n",
    "        links[count,0]= split[0]\n",
    "        links[count,1]= split[2]\n",
    "        count+=1\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create an actor links list\n",
    "###################################\n",
    "actorsLinks = []\n",
    "files = io.open(\"obj/actorsLinksList.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "for i in range(count-1):\n",
    "    j = i+1\n",
    "    while (j<count) and (links[i,0]==links[j,0]):\n",
    "        actorsLinks.append([links[i,1],links[j,1],links[i,0]]) #[actor1, actor2, movie]\n",
    "        files.write(str(links[i,1])+\"\\t\"+str(links[j,1])+\"\\t\"+links[i,0]+\"\\r\\n\")\n",
    "        j+=1\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD & CLEAN DATA FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLoadData():\n",
    "    mDF = pd.read_pickle(\"obj/moviesDF.pkl\")\n",
    "    aDF = pd.read_pickle(\"obj/actorsDF.pkl\")\n",
    "    aLL = []\n",
    "    files = io.open(\"obj/actorsLinksList.txt\", mode=\"r\", encoding=\"utf-8\")\n",
    "    for row in files:\n",
    "        split = row.split(\"\\t\")\n",
    "        aLL.append(split)\n",
    "    files.close()\n",
    "    return mDF,aDF,aLL\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been cleaned and saved into files, all there is left to do is load the data and use it in the rest of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned data stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the \"What is our data set\" chapter the original data consists of over 30 million rows and 1.3 Gb of data.\n",
    "The cleaned data ends up being around 44.000 rows with a size of 2.1Mb. which is approximately 0,15% of the original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
