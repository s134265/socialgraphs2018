{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is our data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project it was decided to work with IMDB data. \n",
    "IMDB provides a subset of their data to everyone who might be interesting in analyzing it. The data provided by IMDB are split into multiple files, not all of these files are used for this project.\n",
    "The files used for this project is:\n",
    "name.basics.tsv.gz\n",
    "title.akas.tsv.gz\n",
    "title.basics.tsv.gz\n",
    "title.crew.tsv.gz\n",
    "title.episode.tsv.gz\n",
    "title.principals.tsv.gz\n",
    "title.ratings.tsv.gz\n",
    "The file \"title.principals\" is the main datafile, which orignally consists of 1.3 Gb of data with 30.674.812 rows and 6 columns.\n",
    "These datafiles contains information about movies, actors in those movies, when they movies were made, what the different peoples roles were in the movies, ID of movies and actors, type of movie such as tv show or movie and ratings of these movies.\n",
    "\n",
    "Besides these data files, some files containing reviews of movies were also used. These reviews were downloaded from the website Kaggle.com and consists of 100.000 reviews on 14.127 movies.\n",
    "The reviews from Kaggle are divided into two folders, each containing 50.000 reviews. Sentiment analysis has already been conducted on some of the reviews, however this was ignored for this project. \n",
    "Besides the moview reviews the data also contains URLS describing which movie the different reviews come from, this is the part linking the movies with the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why did you chose this/ these particular datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets chosen for this project provides with a way to link actors to each other through their movies. Besides this it also provides reviews for the movies, such that sentiment analysis can be done on these in order to link the sentiment score of the movies actors has been in, to the actors. \n",
    "\n",
    "Some of the data files are also used when cleaning the data and making it more suitable for the project, this was especially important since the data set was very large initially.\n",
    "\n",
    "All of these data files therefore provides everything needed in order to make this project, which is why they were chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to find communities of actors/actresses which are enjoyable together. The project is therefore not about find good movies, but instead finding out which actors/actresses make good movies when working together.\n",
    "\n",
    "It is therefore possible for an actor to have bad reviews in general, but still being enjoyable to watch when paired up with certain actors/actresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic stast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two type of data:\n",
    "* reviews\n",
    "* IMBD databases containing actors, movies and rating <br>\n",
    "\n",
    "The databases contains alot of irrelevant information such as games and movies with no reviews in the review data set. Therefore we first have to clean our databases in order to keep only the relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PICKLE\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Initialise a movie dictionnary\n",
    "###################################\n",
    "\n",
    "# Function. to convert movie or actor id to sting key\n",
    "def idToString(iD, base): # base = \"tt\" for movies or \"nm\" for actors\n",
    "    if iD<10:\n",
    "        return base+\"000000\"+str(iD)\n",
    "    if iD<100:\n",
    "        return base+\"00000\"+str(iD)\n",
    "    if iD<1000:\n",
    "        return base+\"0000\"+str(iD)\n",
    "    if iD<10000:\n",
    "        return base+\"000\"+str(iD)\n",
    "    if iD<100000:\n",
    "        return base+\"00\"+str(iD)\n",
    "    if iD<1000000:\n",
    "        return base+\"0\"+str(iD)\n",
    "    else:\n",
    "        return base+str(iD)\n",
    "    \n",
    "# Create movie dictionnary\n",
    "movieDict = {}\n",
    "lastMovie = 9999999 #last movie ID\n",
    "if not fastExecution:\n",
    "    for i in range(lastMovie):\n",
    "        movieDict[idToString(i+1,\"tt\")] = False\n",
    "    print \"Movie Dictionnary initialised\"\n",
    "else:\n",
    "    print \"Fast execution mode, movie dictionnary will be initialised later\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially all entries in the movieDict is set to False. In order to see which movies has reviews, each movie ID with at least one review were set to True instead.\n",
    "A dictionary for actors are also created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Get the movies to keep\n",
    "###################################\n",
    "\n",
    "# List of the reviews documents\n",
    "listReviewsDocuments = [\"train/urls_neg.txt\",\"test/urls_neg.txt\",\"train/urls_pos.txt\",\"test/urls_pos.txt\",\"train/urls_unsup.txt\"]\n",
    "\n",
    "# Fill in the dictionnary\n",
    "for document in listReviewsDocuments:\n",
    "    files = io.open(\"aclImdb/\"+document, mode=\"r\", encoding=\"utf-8\")\n",
    "    for row in files:\n",
    "        w = re.findall(r'http://www.imdb.com/title/(\\w*)/usercomments',row)\n",
    "        movieDict[w[0]] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create an Actor Dict\n",
    "###################################\n",
    "actorDict = {}\n",
    "lastActor = 29999999 #last movie ID\n",
    "for i in range(lastActor):\n",
    "    actorDict[idToString(i+1,\"nm\")] = False\n",
    "print \"Actor Dictionnary initialised\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this setup the data is ready to be cleaned.\n",
    "First thing to do is to only keep movies which has reviews. This was done by keeping movies with \"True\" as their value in the movieDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# key to movie name file\n",
    "###################################\n",
    "\n",
    "if not startFromCleanData:\n",
    "    path = \"DATA/title.basics.txt\"\n",
    "    cleanPath = \"DATA/title.basics.clean.txt\"\n",
    "    files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    cleanfile = io.open(cleanPath, mode=\"w\", encoding=\"utf-8\")\n",
    "    b=False # skip the first line\n",
    "    count =0\n",
    "    for row in files:\n",
    "        if b:\n",
    "            split=row.split(\"\\t\")\n",
    "            key = split[0]\n",
    "            if movieDict[key]:\n",
    "                if (split[1] in ['movie', 'tvMovie']):\n",
    "                    cleanfile.write(row)\n",
    "                    count +=1\n",
    "                else:\n",
    "                    movieDict[key]=False\n",
    "        else:\n",
    "            b=True\n",
    "    files.close()\n",
    "    cleanfile.close()\n",
    "\n",
    "\n",
    "    print \"There are \"+str(count)+\" movies considered\"\n",
    "    print \"DATA/title.basics.txt cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this step, only actors and actresses in the remaining movies should be saved, everyone not in the movies or with another role than actor/actress where therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# film actors links file : Clean + get actor dictionnary\n",
    "##########################################################\n",
    "\n",
    "if not startFromCleanData:\n",
    "    path = \"DATA/title.principals.txt\"\n",
    "    cleanPath = \"DATA/title.principals.clean.txt\"\n",
    "    files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    cleanfile = io.open(cleanPath, mode=\"w\", encoding=\"utf-8\")\n",
    "    roleCheckList = [\"actor\", \"actress\", \"self\"] #check if it is an actor\n",
    "    nLinks = 0\n",
    "    i=False # skip first line\n",
    "    for row in files:\n",
    "        if i:\n",
    "            split = row.split(\"\\t\") \n",
    "            key = split[0]\n",
    "            if movieDict[key]:\n",
    "                if (split[3] in roleCheckList or split[4] in roleCheckList or split[5] in roleCheckList):\n",
    "                    cleanfile.write(row)\n",
    "                    actorDict[split[2]]=True\n",
    "                    nLinks  +=1\n",
    "\n",
    "        else:\n",
    "            i=True\n",
    "\n",
    "    files.close()\n",
    "    cleanfile.close()\n",
    "\n",
    "    ##REMOVE ERRORS\n",
    "    actorDict[\"nm0547707\"]=False\n",
    "    actorDict['nm0547707']=False\n",
    "    actorDict['nm0809728']=False\n",
    "    actorDict['nm2442859']=False\n",
    "    actorDict['nm1996613']=False\n",
    "    actorDict['nm0600636']=False\n",
    "    actorDict['nm1824417']=False\n",
    "    actorDict['nm2440192']=False\n",
    "    actorDict['nm1754167']=False\n",
    "\n",
    "    print \"There are \"+str(nLinks-9)+\" actors considered\"\n",
    "    print \"DATA/title.principals.txt cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# key to actor name file\n",
    "###################################\n",
    "\n",
    "if not startFromCleanData:\n",
    "    path = \"DATA/name.basics.txt\"\n",
    "    cleanPath = \"DATA/name.basics.clean.txt\"\n",
    "    files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    cleanfile = io.open(cleanPath, mode=\"w\", encoding=\"utf-8\")\n",
    "    count = 0\n",
    "    i=False\n",
    "    for row in files:\n",
    "        if i:\n",
    "            split = row.split(\"\\t\")\n",
    "            key = split[0]\n",
    "            if actorDict[key]:\n",
    "                cleanfile.write(row)\n",
    "        else:\n",
    "            i=True\n",
    "\n",
    "    files.close()\n",
    "    cleanfile.close()\n",
    "    print \"DATA/name.basics.txt cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialised clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything not relevant for the project has been removed and only relevant movies and actors/acresses remain, it is then necessary to initialise all of this data, in order to gather relevant information about the data such as movie years etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Preprocess Movie Dict and get movie years\n",
    "############################################\n",
    "\n",
    "movieAgeDict = {}\n",
    "\n",
    "path = \"DATA/title.basics.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count =0\n",
    "for row in files:\n",
    "    split=row.split(\"\\t\")\n",
    "    key = split[0]\n",
    "    if movieDict[key]:\n",
    "        if (split[1] in ['movie', 'tvMovie']) and not (split[5] == \"\\\\N\"):\n",
    "            movieAgeDict[key] = int(split[5])\n",
    "            count +=1\n",
    "files.close()\n",
    "\n",
    "#Clean Movie dict\n",
    "for i in range(lastMovie):\n",
    "    movieDict[idToString(i+1,\"tt\")] = False\n",
    "\n",
    "for key in movieAgeDict.keys():\n",
    "    movieDict[key]=True\n",
    "\n",
    "\n",
    "print \"There are \"+str(count)+\" movies considered\"\n",
    "print \"Movie Dictionnary Preprocessed and Movie Age Dictionnary Built\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# film actors links file : Clean + get actor dictionnary\n",
    "##########################################################\n",
    "\n",
    "path = \"DATA/title.principals.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "roleCheckList = [\"actor\", \"actress\", \"self\"] #check if it is an actor\n",
    "nLinks = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\") \n",
    "    key = split[0]\n",
    "    if movieDict[key]:\n",
    "        if (split[3] in roleCheckList or split[4] in roleCheckList or split[5] in roleCheckList):\n",
    "            actorDict[split[2]]=True\n",
    "            nLinks  +=1\n",
    "\n",
    "files.close()\n",
    "\n",
    "###REMOVE ERRORS\n",
    "actorDict[\"nm0547707\"]=False\n",
    "actorDict['nm0547707']=False\n",
    "actorDict['nm0809728']=False\n",
    "actorDict['nm2442859']=False\n",
    "actorDict['nm1996613']=False\n",
    "actorDict['nm0600636']=False\n",
    "actorDict['nm1824417']=False\n",
    "actorDict['nm2440192']=False\n",
    "actorDict['nm1754167']=False\n",
    "\n",
    "print \"There are \"+str(nLinks-9)+\" actors considered\"\n",
    "\n",
    "print \"Actor Dictionnary Preprocessed\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a ratings dict\n",
    "###################################\n",
    "ratingDict = {}\n",
    "path = \"DATA/ratings.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count = 0\n",
    "i=False # skip first line\n",
    "for row in files:\n",
    "    if i:\n",
    "        key = row[:9]\n",
    "        if movieDict[key]:\n",
    "            split = row.split(\"\\t\") \n",
    "            ratingDict[key] = float(split[1])\n",
    "    else:\n",
    "        i=True\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a movie name dict\n",
    "###################################\n",
    "movieNameDict = {}\n",
    "moviesList = []\n",
    "path = \"DATA/title.akas.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\") \n",
    "    if movieDict[split[0]] and not (split[0] in movieNameDict) and (split[0] in ratingDict) and \"original\" in row   :\n",
    "        movieNameDict[split[0]] = split[2]\n",
    "        moviesList.append(split[0])\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create an actor name dict\n",
    "###################################\n",
    "actorNameDict = {}\n",
    "actorGenderDict = {}\n",
    "actorsList = []\n",
    "path = \"DATA/name.basics.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "count = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\") \n",
    "    if actorDict[split[0]] and not (split[0] in actorNameDict):\n",
    "        actorNameDict[split[0]] = split[1]\n",
    "        if \"actor\" in split[4]:\n",
    "            actorGenderDict[split[0]] = \"M\"\n",
    "        else:\n",
    "            actorGenderDict[split[0]] = \"F\"\n",
    "        actorsList.append(split[0])\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Build a movie data frame\n",
    "###################################\n",
    "if not fastExecution:\n",
    "    moviesData = {\"iD\" : movieNameDict.keys(), \"Title\": pd.Series(np.zeros(len(moviesList))), \"Rating\":pd.Series(np.zeros(len(moviesList))), \"Year\":pd.Series(np.zeros(len(moviesList)))}\n",
    "    moviesDF = pd.DataFrame(moviesData)\n",
    "    for i in moviesDF.index:\n",
    "        iD =moviesDF.loc[i].at[\"iD\"]\n",
    "        moviesDF.loc[i, \"Title\"]= movieNameDict[iD]\n",
    "        moviesDF.loc[i, \"Rating\"] = ratingDict[iD]\n",
    "        moviesDF.loc[i, \"Year\"]= movieAgeDict[iD]\n",
    "    if savingFigures:\n",
    "        moviesDF.to_pickle(\"obj/moviesDF.pkl\")\n",
    "else:\n",
    "    moviesDF = pd.read_pickle(\"obj/moviesDF.pkl\")\n",
    "moviesDF.sort_values(\"Rating\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Build an actor data frame\n",
    "###################################\n",
    "if not fastExecution:\n",
    "    actorsData = {\"iD\": actorNameDict.keys(), \"Name\": pd.Series(np.zeros(len(actorsList))),\"Gender\": pd.Series(np.zeros(len(actorsList)))}\n",
    "    actorsDF = pd.DataFrame(actorsData)\n",
    "    for i in actorsDF.index:\n",
    "        iD = actorsDF.loc[i].at[\"iD\"]\n",
    "        actorsDF.loc[i, \"Name\"]= actorNameDict[iD]\n",
    "        actorsDF.loc[i, \"Gender\"] = actorGenderDict[iD]\n",
    "    if savingFigures:\n",
    "        actorsDF.to_pickle(\"obj/actorsDF.pkl\")\n",
    "else:\n",
    "    actorsDF = pd.read_pickle(\"obj/actorsDF.pkl\")\n",
    "actorsDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create a links list\n",
    "###################################\n",
    "path = \"DATA/title.principals.clean.txt\"\n",
    "files = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "links = np.empty((nLinks,2),dtype=object)\n",
    "count = 0\n",
    "for row in files:\n",
    "    split = row.split(\"\\t\")\n",
    "    if actorDict[split[2]]:\n",
    "        links[count,0]= split[0]\n",
    "        links[count,1]= split[2]\n",
    "        count+=1\n",
    "\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# Create an actor links list\n",
    "###################################\n",
    "actorsLinks = []\n",
    "files = io.open(\"obj/actorsLinksList.txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "for i in range(count-1):\n",
    "    j = i+1\n",
    "    while (j<count) and (links[i,0]==links[j,0]):\n",
    "        actorsLinks.append([links[i,1],links[j,1],links[i,0]]) #[actor1, actor2, movie]\n",
    "        files.write(str(links[i,1])+\"\\t\"+str(links[j,1])+\"\\t\"+links[i,0]+\"\\r\\n\")\n",
    "        j+=1\n",
    "files.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLoadData():\n",
    "    mDF = pd.read_pickle(\"obj/moviesDF.pkl\")\n",
    "    aDF = pd.read_pickle(\"obj/actorsDF.pkl\")\n",
    "    aLL = []\n",
    "    files = io.open(\"obj/actorsLinksList.txt\", mode=\"r\", encoding=\"utf-8\")\n",
    "    for row in files:\n",
    "        split = row.split(\"\\t\")\n",
    "        aLL.append(split)\n",
    "    files.close()\n",
    "    return mDF,aDF,aLL\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned data stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the \"What is our data set\" chapter the original data consists of over 30 million rows and 1.3 Gb of data.\n",
    "The cleaned data ends up being around 44.000 rows with a size of 2.1Mb. which is around 0,15% of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
