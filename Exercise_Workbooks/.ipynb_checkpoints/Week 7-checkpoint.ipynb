{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3.12 6 and 3.12 30\n",
    "\n",
    "6:\n",
    "Describe the class of strings matched by the following regular expressions.\n",
    "\n",
    "[a-zA-Z]+   \n",
    "  Means any letter (lower or upper case) with 1 or more repetitions. so aBwasaSwA would be accepted.   \n",
    "\n",
    "[A-Z][a-z]*\n",
    "    One upper case letter followed by any amount of lower case.so A is accepted and All is accepted, but AaA is not.\n",
    "\n",
    "p[aeiou]{,2}t\n",
    "    This means that a word should start with \"p\" then one of the letters \"aeiou\" between 0 and 2 times and then t. \n",
    "    So pet is accepted and pt is accepted etc.\n",
    "\n",
    "\n",
    "\\d+(\\.\\d+)?\n",
    "    \\d is short for [0-9] and \\. is just a \".\" therefor this accepts any digits between 0-9 1 or more times, then a dot and 0-9 repeated 1 or more times however the ? means 0 or 1 of the previous signs (the thing in the capture) so it accepts Just 9 or 99 or 925 or 9.912 etc.\n",
    "\n",
    "([^aeiou][aeiou][^aeiou])*\n",
    "    The ^ inside the brackets means all other letters than these, so this would accept bab for example or babbab\n",
    "    But it would not accept aab since the first letter must not be a.\n",
    "\n",
    "\\w+|[^\\w\\s]+\n",
    "        \\w means [a-zA-Z0-9] so all leters and numbers. im guessing the next part means all other letters than those in \\w and           \\s is whitespaces and newlines so not those either.\n",
    "        Therefore aW9. would be accepted but AW9a would not since the plus means 1 or more things which is not a letter/ number/         whitespace.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use stemmers:\n",
    "import nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "text = \"This is a test which the stemmers should be used upon, it tests the programs and figures out what to do. There are a figure in the book which describes it\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can be seen that even with this small text they are different, the porter stemmer has alot of unicode, the lancaster does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download all the wiki pages, with the new \"content\" format.\n",
    "\n",
    "import urllib2\n",
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=extracts&exlimit=max&explaintext\"\n",
    "rvprop =\"rvprop=timestamp|content\"\n",
    "dataformat = \"format=json\"\n",
    "rvdir = \"rvdir=older\" #sort revisions from newest to oldest\n",
    "start = \"rvend=2016-01-03T00:00:00Z\" #start of my time period\n",
    "end = \"rvstart=2020-01-03T00:00:00Z\" #end of my time period\n",
    "limit = \"rvlimit=1\" #consider only the first revision\n",
    "wikiresponse = {}\n",
    "\n",
    "df3 = pd.read_csv('H115.csv')\n",
    "df3['congress_number'] =115\n",
    "\n",
    "\n",
    "members = df3['WikiPageName'].unique()\n",
    "for member in members:\n",
    "    title = \"titles=\" + member\n",
    "    query = \"%s%s&%s&%s&%s&%s&%s&%s&%s&%s\" % (baseurl, action, title, content, rvprop, dataformat, rvdir, end, start, limit)\n",
    "    wikisource = urllib2.urlopen(query)\n",
    "    Temp = df3.loc[df3['WikiPageName'] == member]\n",
    "    \n",
    "    w = wikisource.read()\n",
    "\n",
    "    wikiJSON = json.loads(w)\n",
    "\n",
    "    \n",
    "    counter = Temp['Party'].str.contains('Republican').sum()\n",
    "    if counter > 0:\n",
    "        file = open('Republican.txt', 'a+')\n",
    "        file.write(str(wikiJSON['query'])) \n",
    "        file.close()\n",
    "    \n",
    "    counter = Temp['Party'].str.contains('Democratic').sum()\n",
    "    if counter > 0:\n",
    "        file = open('Democratic.txt', 'a+')\n",
    "        file.write(str(wikiJSON['query'])) \n",
    "        file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "file = open('Republican.txt', 'r')\n",
    "text = file.read()\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "text = text.lower()\n",
    "\n",
    "members2 = members\n",
    "\n",
    "for i in range (0, len(members2)):\n",
    "    members2[i] = members2[i].lower()\n",
    "\n",
    "for member in members:\n",
    "    split = member.split(\"_\")\n",
    "    for i in range(0,2):\n",
    "        members2 = np.append(members2, split[i].lower())\n",
    "                                                        \n",
    "filtered_Republican = tokenizer.tokenize(text)\n",
    "\n",
    "members2 = np.append(members2, 'n')\n",
    "members2 = np.append(members2, 'u')\n",
    "\n",
    "stopwordslist = set(stopwords.words('english'))\n",
    "members2 = set(members2)\n",
    "\n",
    "filtered_Republican = [w for w in filtered_Republican if w not in stopwordslist and w not in members2]\n",
    "\n",
    "filtered_Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Democratic.txt', 'r')\n",
    "text = file.read()\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "text = text.lower()\n",
    "\n",
    "filtered_Democratic = tokenizer.tokenize(text)\n",
    "\n",
    "filtered_Democratic = [w for w in filtered_Democratic if w not in stopwordslist and w not in members2]\n",
    "\n",
    "filtered_Democratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_Republican))\n",
    "print(len(filtered_Democratic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(filtered_Republican)\n",
    "print count.most_common(5)\n",
    "\n",
    "count2 = Counter(filtered_Democratic)\n",
    "print count2.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "republicanTF =  Counter({k:float(v)/float(len(filtered_Republican)) for k,v in count.items()})\n",
    "democraticTF =  Counter({k:float(v)/float(len(filtered_Democratic)) for k,v in count2.items()})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print republicanTF.most_common(5)\n",
    "print democraticTF.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "republicanIDF = Counter()\n",
    "democraticIDF = Counter()\n",
    "republicanTFIDF = Counter()\n",
    "democraticTFIDF = Counter()\n",
    "\n",
    "import math\n",
    "\n",
    "for keys in count:\n",
    "    c = 1\n",
    "    if keys in count2.keys():\n",
    "        c = 2        \n",
    "    republicanIDF[keys] = (math.log(float(2)/float(c)))\n",
    "    republicanTFIDF[keys] = republicanTF[keys]*republicanIDF[keys]\n",
    "\n",
    "for keys in count2:\n",
    "    c = 1\n",
    "    if keys in count.keys():\n",
    "        c = 2        \n",
    "    democraticIDF[keys] = (math.log(float(2)/float(c)))\n",
    "    democraticTFIDF[keys] = democraticTF[keys]*democraticIDF[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print republicanTFIDF.most_common(10)\n",
    "print democraticTFIDF.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(background_color='white')\n",
    "wordcloud.generate_from_frequencies(frequencies=republicanTFIDF)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white')\n",
    "wordcloud.generate_from_frequencies(frequencies=democraticTFIDF)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
